{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27ebae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, Bidirectional, GRU, Dense, Concatenate\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71396074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f765325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d142ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing functions\n",
    "def remove_special_chars(text):\n",
    "    # Remove special characters and punctuation\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83f4bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ki_shari\\Downloads\\DFF.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5456deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"LABEL\"] == \"__label1__\", \"LABEL\"] = 1\n",
    "df.loc[df[\"LABEL\"] == \"__label2__\", \"LABEL\"] = 0\n",
    "df['LABEL']=pd.to_numeric(df['LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e04cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing steps\n",
    "df['ORIGINAL_TEXT'] = df['ORIGINAL_TEXT'].apply(remove_special_chars)\n",
    "df['ORIGINAL_TEXT'] = df['ORIGINAL_TEXT'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fab08709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X = df['ORIGINAL_TEXT'].values\n",
    "y = df['LABEL'].values\n",
    "\n",
    "# Convert labels to binary format (0, 1)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff299f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for tokenization and padding\n",
    "max_features = 10000  # Maximum number of words to keep based on word frequency\n",
    "maxlen = 100  # Maximum length of each review (truncate or pad with zeros)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad the sequences\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=maxlen)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 300\n",
    "embedding_path = 'C:\\\\Users\\\\ki_shari\\\\Downloads\\\\glove.6B.300d.txt\\\\glove.6B.300d.txt'\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "with open(embedding_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, vec = line.split(' ', 1)\n",
    "        if word in tokenizer.word_index and tokenizer.word_index[word] < max_features:\n",
    "            embedding_matrix[tokenizer.word_index[word]] = np.fromstring(vec, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd8b827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 13s 90ms/step\n",
      "132/132 [==============================] - 14s 109ms/step\n",
      "132/132 [==============================] - 12s 93ms/step\n",
      "132/132 [==============================] - 16s 122ms/step\n",
      "132/132 [==============================] - 13s 101ms/step\n",
      "Accuracy: 0.8595714285714285\n",
      "F1 Score: 0.858105313811819\n",
      "Recall: 0.8554285714285715\n",
      "Precision: 0.8609623058905225\n",
      "AUC: 0.9089017006802722\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Bidirectional, GRU, Dense\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "input1 = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "gru_layer = Bidirectional(GRU(64))(embedding_layer)\n",
    "output = Dense(1, activation='sigmoid')(gru_layer)\n",
    "\n",
    "model = Model(inputs=input1, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = np.round(y_pred).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb37230",
   "metadata": {},
   "outputs": [],
   "source": [
    "####CNN+BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef828f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 17s 99ms/step\n",
      "132/132 [==============================] - 15s 115ms/step\n",
      "132/132 [==============================] - 14s 107ms/step\n",
      "132/132 [==============================] - 16s 119ms/step\n",
      "132/132 [==============================] - 15s 116ms/step\n",
      "Average Metrics:\n",
      "Accuracy: 0.8111428571428572\n",
      "Precision: 0.8217282210176166\n",
      "Recall: 0.8135238095238094\n",
      "F1-Score: 0.8156437380161308\n",
      "AUC: 0.8111428571428572\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(vocab_size, embedding_dim, maxlen):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
    "    conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
    "    conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "    \n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "    \n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3])\n",
    "    \n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "    \n",
    "    model = Model(inputs=input_text, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Define the BiGRU model\n",
    "input1 = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "gru_layer = Bidirectional(GRU(64))(embedding_layer)\n",
    "output = Dense(1, activation='sigmoid')(gru_layer)\n",
    "\n",
    "model = Model(inputs=input1, outputs=output)\n",
    "\n",
    "# Combine the models\n",
    "combined_input = Input(shape=(maxlen,))\n",
    "cnn_model = create_cnn_model(max_features, embedding_dim, maxlen)(combined_input)\n",
    "gru_model = model(combined_input)\n",
    "\n",
    "concatenated = Concatenate()([cnn_model, gru_model])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "hybrid_model = Model(inputs=combined_input, outputs=output)\n",
    "\n",
    "# Compile the hybrid model\n",
    "hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Define the adaptive learning rate callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_padded, y):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the hybrid model\n",
    "    hybrid_model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0, callbacks=[reduce_lr], validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = hybrid_model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy, precision, recall, f1, auc = evaluate_metrics(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Calculate average metrics\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1 = np.mean(f1_scores)\n",
    "average_auc = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Metrics:\")\n",
    "print(f\"Accuracy: {average_accuracy}\")\n",
    "print(f\"Precision: {average_precision}\")\n",
    "print(f\"Recall: {average_recall}\")\n",
    "print(f\"F1-Score: {average_f1}\")\n",
    "print(f\"AUC: {average_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d75998",
   "metadata": {},
   "outputs": [],
   "source": [
    "####CNN+BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06576966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 12s 86ms/step\n",
      "132/132 [==============================] - 14s 108ms/step\n",
      "132/132 [==============================] - 15s 111ms/step\n",
      "132/132 [==============================] - 14s 107ms/step\n",
      "132/132 [==============================] - 14s 105ms/step\n",
      "Average Metrics:\n",
      "Accuracy: 0.8149047619047618\n",
      "Precision: 0.820073064625516\n",
      "Recall: 0.8078095238095238\n",
      "F1-Score: 0.8135864794501128\n",
      "AUC: 0.8149047619047618\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, LSTM\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(vocab_size, embedding_dim, maxlen):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
    "    conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
    "    conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "    \n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "    \n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3])\n",
    "    \n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "    \n",
    "    model = Model(inputs=input_text, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Define the BiLSTM model\n",
    "input1 = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "lstm_layer = Bidirectional(LSTM(64))(embedding_layer)\n",
    "output = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input1, outputs=output)\n",
    "\n",
    "# Combine the models\n",
    "combined_input = Input(shape=(maxlen,))\n",
    "cnn_model = create_cnn_model(max_features, embedding_dim, maxlen)(combined_input)\n",
    "lstm_model = model(combined_input)\n",
    "\n",
    "concatenated = Concatenate()([cnn_model, lstm_model])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "hybrid_model = Model(inputs=combined_input, outputs=output)\n",
    "\n",
    "# Compile the hybrid model\n",
    "hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Define the adaptive learning rate callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_padded, y):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the hybrid model\n",
    "    hybrid_model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0, callbacks=[reduce_lr], validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = hybrid_model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy, precision, recall, f1, auc = evaluate_metrics(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Calculate average metrics\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1 = np.mean(f1_scores)\n",
    "average_auc = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Metrics:\")\n",
    "print(f\"Accuracy: {average_accuracy}\")\n",
    "print(f\"Precision: {average_precision}\")\n",
    "print(f\"Recall: {average_recall}\")\n",
    "print(f\"F1-Score: {average_f1}\")\n",
    "print(f\"AUC: {average_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BiGRU+gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be8586ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ki_shari\\AppData\\Local\\Temp\\ipykernel_15944\\4144731606.py:26: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=16, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 35s 257ms/step\n",
      "132/132 [==============================] - 34s 256ms/step\n",
      "132/132 [==============================] - 34s 261ms/step\n",
      "132/132 [==============================] - 34s 254ms/step\n",
      "132/132 [==============================] - 34s 257ms/step\n",
      "Best Hyperparameters:\n",
      "Dropout Rate: 0.2\n",
      "GRU Units: 128\n",
      "Accuracy: 0.8752380952380954\n",
      "F1 Score: 0.8788474229083162\n",
      "Recall: 0.8908571428571428\n",
      "Precision: 0.8675390121611393\n",
      "AUC: 0.9165354648526078\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Bidirectional, GRU, Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(dropout_rate=0.0, gru_units=64):\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "    gru_layer = Bidirectional(GRU(gru_units, dropout=dropout_rate))(embedding_layer)\n",
    "    dropout_layer = Dropout(dropout_rate)(gru_layer)\n",
    "    output = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Perform grid search for hyperparameters\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'gru_units': [32, 64, 128],\n",
    "}\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=16, verbose=0)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(X_padded, df['LABEL'])\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_params = grid_result.best_params_\n",
    "best_dropout_rate = best_params['dropout_rate']\n",
    "best_gru_units = best_params['gru_units']\n",
    "\n",
    "best_model = create_model(dropout_rate=best_dropout_rate, gru_units=best_gru_units)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    best_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                   batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_binary = np.round(y_pred).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Dropout Rate:\", best_dropout_rate)\n",
    "print(\"GRU Units:\", best_gru_units)\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN+BiGRU+gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d5af800",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_63'), name='input_63', description=\"created by layer 'input_63'\") at layer \"embedding_57\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m Concatenate()([cnn_model, gru_model])\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(concatenated)\n\u001b[1;32m---> 39\u001b[0m hybrid_model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Compile the hybrid model\u001b[39;00m\n\u001b[0;32m     42\u001b[0m hybrid_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\functional.py:167\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    159\u001b[0m         [\n\u001b[0;32m    160\u001b[0m             functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    162\u001b[0m         ]\n\u001b[0;32m    163\u001b[0m     ):\n\u001b[0;32m    164\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(\n\u001b[0;32m    165\u001b[0m             inputs, outputs\n\u001b[0;32m    166\u001b[0m         )\n\u001b[1;32m--> 167\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\functional.py:266\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\functional.py:1142\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[1;32m-> 1142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph disconnected: cannot obtain value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1145\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following previous layers were accessed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1146\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout issue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_with_complete_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[0;32m   1149\u001b[0m     computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_63'), name='input_63', description=\"created by layer 'input_63'\") at layer \"embedding_57\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(vocab_size, embedding_dim, maxlen, kernel_size=3, filters=128, dense_units=64, dropout_rate=0.0):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    \n",
    "    pooling = GlobalMaxPooling1D()(batchnorm)\n",
    "    \n",
    "    dense = Dense(dense_units, activation='relu')(pooling)\n",
    "    dropout = Dropout(dropout_rate)(dense)\n",
    "    \n",
    "    return dropout\n",
    "\n",
    "# Define the BiGRU model\n",
    "def create_gru_model(embedding_dim, maxlen, gru_units=64, dropout_rate=0.0):\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "    gru_layer = Bidirectional(GRU(gru_units))(embedding_layer)\n",
    "    dropout = Dropout(dropout_rate)(gru_layer)\n",
    "    return dropout\n",
    "\n",
    "# Combine the models\n",
    "combined_input = Input(shape=(maxlen,))\n",
    "cnn_model = create_cnn_model(max_features, embedding_dim, maxlen)\n",
    "gru_model = create_gru_model(embedding_dim, maxlen)\n",
    "concatenated = Concatenate()([cnn_model, gru_model])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "hybrid_model = Model(inputs=combined_input, outputs=output)\n",
    "\n",
    "# Compile the hybrid model\n",
    "hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Define the adaptive learning rate callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Perform grid search for hyperparameters\n",
    "param_grid = {\n",
    "    'kernel_size': [3, 4, 5],\n",
    "    'filters': [64, 128, 256],\n",
    "    'dense_units': [32, 64],\n",
    "    'gru_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=hybrid_model, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(X_padded, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_kernel_size = grid_result.best_params_['kernel_size']\n",
    "best_filters = grid_result.best_params_['filters']\n",
    "best_dense_units = grid_result.best_params_['dense_units']\n",
    "best_gru_units = grid_result.best_params_['gru_units']\n",
    "best_dropout_rate = grid_result.best_params_['dropout_rate']\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_padded, y):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the hybrid model with the best hyperparameters\n",
    "    cnn_model = create_cnn_model(max_features, embedding_dim, maxlen, kernel_size=best_kernel_size, filters=best_filters, dense_units=best_dense_units, dropout_rate=best_dropout_rate)\n",
    "    gru_model = create_gru_model(embedding_dim, maxlen, gru_units=best_gru_units, dropout_rate=best_dropout_rate)\n",
    "    concatenated = Concatenate()([cnn_model, gru_model])\n",
    "    output = Dense(1, activation='sigmoid')(concatenated)\n",
    "    hybrid_model = Model(inputs=combined_input, outputs=output)\n",
    "    hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    hybrid_model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0, callbacks=[reduce_lr], validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = hybrid_model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy, precision, recall, f1, auc = evaluate_metrics(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Calculate average metrics\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1 = np.mean(f1_scores)\n",
    "average_auc = np.mean(auc_scores)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Kernel Size:\", best_kernel_size)\n",
    "print(\"Filters:\", best_filters)\n",
    "print(\"Dense Units:\", best_dense_units)\n",
    "print(\"GRU Units:\", best_gru_units)\n",
    "print(\"Dropout Rate:\", best_dropout_rate)\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f\"Accuracy: {average_accuracy}\")\n",
    "print(f\"Precision: {average_precision}\")\n",
    "print(f\"Recall: {average_recall}\")\n",
    "print(f\"F1-Score: {average_f1}\")\n",
    "print(f\"AUC: {average_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN+BiLSTM+gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a1ab958",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_66'), name='input_66', description=\"created by layer 'input_66'\") at layer \"embedding_59\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m Concatenate()([cnn_model, gru_model])\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(concatenated)\n\u001b[1;32m---> 39\u001b[0m hybrid_model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Compile the hybrid model\u001b[39;00m\n\u001b[0;32m     42\u001b[0m hybrid_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\functional.py:167\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    159\u001b[0m         [\n\u001b[0;32m    160\u001b[0m             functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    162\u001b[0m         ]\n\u001b[0;32m    163\u001b[0m     ):\n\u001b[0;32m    164\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(\n\u001b[0;32m    165\u001b[0m             inputs, outputs\n\u001b[0;32m    166\u001b[0m         )\n\u001b[1;32m--> 167\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\functional.py:266\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\functional.py:1142\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[1;32m-> 1142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph disconnected: cannot obtain value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1145\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following previous layers were accessed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1146\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout issue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_with_complete_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[0;32m   1149\u001b[0m     computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_66'), name='input_66', description=\"created by layer 'input_66'\") at layer \"embedding_59\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(vocab_size, embedding_dim, maxlen, kernel_size=3, filters=128, dense_units=64, dropout_rate=0.0):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    \n",
    "    pooling = GlobalMaxPooling1D()(batchnorm)\n",
    "    \n",
    "    dense = Dense(dense_units, activation='relu')(pooling)\n",
    "    dropout = Dropout(dropout_rate)(dense)\n",
    "    \n",
    "    return dropout\n",
    "\n",
    "# Define the BiGRU model\n",
    "def create_gru_model(embedding_dim, maxlen, gru_units=64, dropout_rate=0.0):\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "    lst_layer = Bidirectional(LSTM(gru_units))(embedding_layer)\n",
    "    dropout = Dropout(dropout_rate)(lst_layer)\n",
    "    return dropout\n",
    "\n",
    "# Combine the models\n",
    "combined_input = Input(shape=(maxlen,))\n",
    "cnn_model = create_cnn_model(max_features, embedding_dim, maxlen)\n",
    "gru_model = create_gru_model(embedding_dim, maxlen)\n",
    "concatenated = Concatenate()([cnn_model, gru_model])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "hybrid_model = Model(inputs=combined_input, outputs=output)\n",
    "\n",
    "# Compile the hybrid model\n",
    "hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Define the adaptive learning rate callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Perform grid search for hyperparameters\n",
    "param_grid = {\n",
    "    'kernel_size': [3, 4, 5],\n",
    "    'filters': [64, 128, 256],\n",
    "    'dense_units': [32, 64],\n",
    "    'gru_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=hybrid_model, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(X_padded, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_kernel_size = grid_result.best_params_['kernel_size']\n",
    "best_filters = grid_result.best_params_['filters']\n",
    "best_dense_units = grid_result.best_params_['dense_units']\n",
    "best_gru_units = grid_result.best_params_['gru_units']\n",
    "best_dropout_rate = grid_result.best_params_['dropout_rate']\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_padded, y):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the hybrid model with the best hyperparameters\n",
    "    cnn_model = create_cnn_model(max_features, embedding_dim, maxlen, kernel_size=best_kernel_size, filters=best_filters, dense_units=best_dense_units, dropout_rate=best_dropout_rate)\n",
    "    gru_model = create_gru_model(embedding_dim, maxlen, lst_units=best_lst_units, dropout_rate=best_dropout_rate)\n",
    "    concatenated = Concatenate()([cnn_model, gru_model])\n",
    "    output = Dense(1, activation='sigmoid')(concatenated)\n",
    "    hybrid_model = Model(inputs=combined_input, outputs=output)\n",
    "    hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    hybrid_model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0, callbacks=[reduce_lr], validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = hybrid_model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy, precision, recall, f1, auc = evaluate_metrics(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Calculate average metrics\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1 = np.mean(f1_scores)\n",
    "average_auc = np.mean(auc_scores)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Kernel Size:\", best_kernel_size)\n",
    "print(\"Filters:\", best_filters)\n",
    "print(\"Dense Units:\", best_dense_units)\n",
    "print(\"LSTM Units:\", best_lst_units)\n",
    "print(\"Dropout Rate:\", best_dropout_rate)\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f\"Accuracy: {average_accuracy}\")\n",
    "print(f\"Precision: {average_precision}\")\n",
    "print(f\"Recall: {average_recall}\")\n",
    "print(f\"F1-Score: {average_f1}\")\n",
    "print(f\"AUC: {average_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e7843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
