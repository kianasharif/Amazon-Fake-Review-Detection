{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d518a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, Bidirectional, GRU, Dense, Concatenate\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be310ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b0b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ki_shari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5937f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing functions\n",
    "def remove_special_chars(text):\n",
    "    # Remove special characters and punctuation\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a286ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ki_shari\\Downloads\\DFF.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a199183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>RATING</th>\n",
       "      <th>VERIFIED_PURCHASE</th>\n",
       "      <th>PRODUCT_CATEGORY</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODUCT_TITLE</th>\n",
       "      <th>REVIEW_TITLE</th>\n",
       "      <th>ORIGINAL_TEXT</th>\n",
       "      <th>normalized_scores</th>\n",
       "      <th>incon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>PC</td>\n",
       "      <td>B00008NG7N</td>\n",
       "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
       "      <td>useful</td>\n",
       "      <td>When least you think so, this product will sav...</td>\n",
       "      <td>0.726538</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>B00LH0Y3NM</td>\n",
       "      <td>Note 3 Battery : Stalion Strength Replacement ...</td>\n",
       "      <td>New era for batteries</td>\n",
       "      <td>Lithium batteries are something new introduced...</td>\n",
       "      <td>0.723638</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Baby</td>\n",
       "      <td>B000I5UZ1Q</td>\n",
       "      <td>Fisher-Price Papasan Cradle Swing, Starlight</td>\n",
       "      <td>doesn't swing very well.</td>\n",
       "      <td>I purchased this swing for my baby. She is 6 m...</td>\n",
       "      <td>0.483963</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>B003822IRA</td>\n",
       "      <td>Casio MS-80B Standard Function Desktop Calculator</td>\n",
       "      <td>Great computing!</td>\n",
       "      <td>I was looking for an inexpensive desk calcolat...</td>\n",
       "      <td>0.726538</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00PWSAXAM</td>\n",
       "      <td>Shine Whitening - Zero Peroxide Teeth Whitenin...</td>\n",
       "      <td>Only use twice a week</td>\n",
       "      <td>I only use it twice a week and the results are...</td>\n",
       "      <td>0.726538</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOC_ID  LABEL  RATING VERIFIED_PURCHASE PRODUCT_CATEGORY  PRODUCT_ID  \\\n",
       "0       1      1       4                 N               PC  B00008NG7N   \n",
       "1       2      1       4                 Y         Wireless  B00LH0Y3NM   \n",
       "2       3      1       3                 N             Baby  B000I5UZ1Q   \n",
       "3       4      1       4                 N  Office Products  B003822IRA   \n",
       "4       5      1       4                 N           Beauty  B00PWSAXAM   \n",
       "\n",
       "                                       PRODUCT_TITLE  \\\n",
       "0        Targus PAUK10U Ultra Mini USB Keypad, Black   \n",
       "1  Note 3 Battery : Stalion Strength Replacement ...   \n",
       "2       Fisher-Price Papasan Cradle Swing, Starlight   \n",
       "3  Casio MS-80B Standard Function Desktop Calculator   \n",
       "4  Shine Whitening - Zero Peroxide Teeth Whitenin...   \n",
       "\n",
       "               REVIEW_TITLE  \\\n",
       "0                    useful   \n",
       "1     New era for batteries   \n",
       "2  doesn't swing very well.   \n",
       "3          Great computing!   \n",
       "4     Only use twice a week   \n",
       "\n",
       "                                       ORIGINAL_TEXT  normalized_scores  \\\n",
       "0  When least you think so, this product will sav...           0.726538   \n",
       "1  Lithium batteries are something new introduced...           0.723638   \n",
       "2  I purchased this swing for my baby. She is 6 m...           0.483963   \n",
       "3  I was looking for an inexpensive desk calcolat...           0.726538   \n",
       "4  I only use it twice a week and the results are...           0.726538   \n",
       "\n",
       "    incon  \n",
       "0  Medium  \n",
       "1  Medium  \n",
       "2     Low  \n",
       "3  Medium  \n",
       "4  Medium  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a95362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        When least you think so, this product will sav...\n",
      "1        Lithium batteries are something new introduced...\n",
      "2        I purchased this swing for my baby. She is 6 m...\n",
      "3        I was looking for an inexpensive desk calcolat...\n",
      "4        I only use it twice a week and the results are...\n",
      "                               ...                        \n",
      "20995    I bought these for work.  I have high arches, ...\n",
      "20996    Crocs are one of only two brands of shoes that...\n",
      "20997    I love moccasins  This fit like it was custom ...\n",
      "20998    I wish these were a little more durable. I got...\n",
      "20999    I've been looking for a replacement for my bel...\n",
      "Name: ORIGINAL_TEXT, Length: 21000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['ORIGINAL_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f011b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"LABEL\"] == \"__label1__\", \"LABEL\"] = 1\n",
    "df.loc[df[\"LABEL\"] == \"__label2__\", \"LABEL\"] = 0\n",
    "df['LABEL']=pd.to_numeric(df['LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10bf603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing steps\n",
    "df['ORIGINAL_TEXT'] = df['ORIGINAL_TEXT'].apply(remove_special_chars)\n",
    "df['ORIGINAL_TEXT'] = df['ORIGINAL_TEXT'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7320301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['incon'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c4d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Medium\n",
      "1        Medium\n",
      "2           Low\n",
      "3        Medium\n",
      "4        Medium\n",
      "          ...  \n",
      "20995    Medium\n",
      "20996    Medium\n",
      "20997      High\n",
      "20998    Medium\n",
      "20999    Medium\n",
      "Name: incon, Length: 21000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['incon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e1f54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'incon' column to numerical values\n",
    "incon_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "df['incon_encoded'] = df['incon'].map(incon_mapping)\n",
    "\n",
    "# Prepare the target labels\n",
    "labels = np.array(df['LABEL'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb0e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [least, think, product, save, day, keep, aroun...\n",
      "1        [lithium, batteri, someth, new, introduc, mark...\n",
      "2        [purchas, swing, babi, 6, month, pretti, much,...\n",
      "3        [look, inexpens, desk, calcolatur, work, every...\n",
      "4        [use, twice, week, result, great, use, teeth, ...\n",
      "                               ...                        \n",
      "20995    [bought, work, high, arch, use, arch, support,...\n",
      "20996    [croc, one, two, brand, shoe, foot, day, work,...\n",
      "20997    [love, moccasin, fit, like, custom, made, mebr...\n",
      "20998    [wish, littl, durabl, got, caught, bolt, cross...\n",
      "20999    [ive, look, replac, belov, kso, trek, own, two...\n",
      "Name: ORIGINAL_TEXT, Length: 21000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['ORIGINAL_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "455cef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X = df['ORIGINAL_TEXT'].values\n",
    "y = df['LABEL'].values\n",
    "incon_encoded = df['incon_encoded'].values\n",
    "normalized_scores = df['normalized_scores'].values\n",
    "\n",
    "\n",
    "# Convert labels to binary format (0, 1)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f18e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for tokenization and padding\n",
    "max_features = 10000  # Maximum number of words to keep based on word frequency\n",
    "maxlen = 100  # Maximum length of each review (truncate or pad with zeros)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad the sequences\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=maxlen)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 300\n",
    "embedding_path = 'C:\\\\Users\\\\ki_shari\\\\Downloads\\\\glove.6B.300d.txt\\\\glove.6B.300d.txt'\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "with open(embedding_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, vec = line.split(' ', 1)\n",
    "        if word in tokenizer.word_index and tokenizer.word_index[word] < max_features:\n",
    "            embedding_matrix[tokenizer.word_index[word]] = np.fromstring(vec, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66a04d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "input1 = Input(shape=(maxlen,))\n",
    "input2 = Input(shape=(1,))  # Input for incon\n",
    "\n",
    "embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "gru_layer = Bidirectional(GRU(64))(embedding_layer)\n",
    "\n",
    "concat_layer = Concatenate()([gru_layer, input2])\n",
    "output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3da41b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 15s 105ms/step\n",
      "132/132 [==============================] - 18s 135ms/step\n",
      "132/132 [==============================] - 16s 124ms/step\n",
      "132/132 [==============================] - 18s 137ms/step\n",
      "132/132 [==============================] - 21s 160ms/step\n",
      "Accuracy: 0.8613809523809524\n",
      "F1 Score: 0.8588240551063532\n",
      "Recall: 0.8536190476190477\n",
      "Precision: 0.8646525287050576\n",
      "AUC: 0.9092801133786848\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['incon_encoded'][train_index], df['incon_encoded'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict([X_test, incon_test])\n",
    "    y_pred_binary = np.round(y_pred).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f37b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####CNN+BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100ca73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 23s 140ms/step\n",
      "132/132 [==============================] - 18s 106ms/step\n",
      "132/132 [==============================] - 17s 103ms/step\n",
      "132/132 [==============================] - 72s 520ms/step\n",
      "132/132 [==============================] - 31s 204ms/step\n",
      "Accuracy: 0.6232380952380951\n",
      "F1 Score: 0.6181938590279368\n",
      "Recall: 0.6105714285714287\n",
      "Precision: 0.626752614962507\n",
      "AUC: 0.6685561451247166\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "def create_hybrid_model(vocab_size, embedding_dim, maxlen):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    input_incon = Input(shape=(1,))\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
    "    conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
    "    conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "\n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3, input_incon])\n",
    "\n",
    "    gru_layer = Bidirectional(GRU(64))(embedding_layer)\n",
    "    concatenated = Concatenate()([gru_layer, concatenated])\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_incon], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['incon_encoded'][train_index], df['incon_encoded'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Create the hybrid model\n",
    "    model = create_hybrid_model(max_features, embedding_dim, maxlen)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict([X_test, incon_test])\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14882e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####CNN+BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86abc6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 18s 104ms/step\n",
      "132/132 [==============================] - 143s 1s/step\n",
      "132/132 [==============================] - 139s 1s/step\n",
      "132/132 [==============================] - 58s 437ms/step\n",
      "132/132 [==============================] - 93s 693ms/step\n",
      "Accuracy: 0.6228571428571429\n",
      "F1 Score: 0.6237693407010736\n",
      "Recall: 0.6260000000000001\n",
      "Precision: 0.6225446198007234\n",
      "AUC: 0.6730128344671202\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, LSTM\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "def create_hybrid_model(vocab_size, embedding_dim, maxlen):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    input_incon = Input(shape=(1,))\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
    "    conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
    "    conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "\n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3, input_incon])\n",
    "\n",
    "    lstm_layer = Bidirectional(LSTM(64))(embedding_layer)\n",
    "    concatenated = Concatenate()([lstm_layer, concatenated])\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_incon], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['incon_encoded'][train_index], df['incon_encoded'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Create the hybrid model\n",
    "    model = create_hybrid_model(max_features, embedding_dim, maxlen)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict([X_test, incon_test])\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####normalized_incon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "015750f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 7s 51ms/step\n",
      "132/132 [==============================] - 7s 52ms/step\n",
      "132/132 [==============================] - 7s 52ms/step\n",
      "132/132 [==============================] - 7s 50ms/step\n",
      "132/132 [==============================] - 7s 53ms/step\n",
      "Accuracy: 0.9021904761904762\n",
      "F1 Score: 0.9007358495925267\n",
      "Recall: 0.9035238095238096\n",
      "Precision: 0.8983013182897231\n",
      "AUC: 0.9258091156462586\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "input1 = Input(shape=(maxlen,))\n",
    "input2 = Input(shape=(1,))  # Input for incon\n",
    "\n",
    "embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "gru_layer = Bidirectional(GRU(128, dropout=0.2))(embedding_layer)\n",
    "\n",
    "concat_layer = Concatenate()([gru_layer, input2])\n",
    "output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['normalized_scores'][train_index], df['normalized_scores'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict([X_test, incon_test])\n",
    "    y_pred_binary = np.round(y_pred).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8de4c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 32s 241ms/step\n",
      "132/132 [==============================] - 23s 171ms/step\n",
      "132/132 [==============================] - 23s 169ms/step\n",
      "132/132 [==============================] - 79s 590ms/step\n",
      "132/132 [==============================] - 47s 351ms/step\n",
      "Accuracy: 0.6192857142857143\n",
      "F1 Score: 0.612259095142868\n",
      "Recall: 0.602\n",
      "Precision: 0.6240971371709229\n",
      "AUC: 0.6656344897959185\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "def create_hybrid_model(vocab_size, embedding_dim, maxlen):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    input_incon = Input(shape=(1,))\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
    "    conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
    "    conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "\n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3, input_incon])\n",
    "\n",
    "    gru_layer = Bidirectional(GRU(64))(embedding_layer)\n",
    "    concatenated = Concatenate()([gru_layer, concatenated])\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_incon], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['normalized_scores'][train_index], df['normalized_scores'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Create the hybrid model\n",
    "    model = create_hybrid_model(max_features, embedding_dim, maxlen)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict([X_test, incon_test])\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf36edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 48s 359ms/step\n",
      "132/132 [==============================] - 116s 871ms/step\n",
      "132/132 [==============================] - 55s 407ms/step\n",
      "132/132 [==============================] - 119s 900ms/step\n",
      "132/132 [==============================] - 85s 640ms/step\n",
      "Accuracy: 0.6214285714285714\n",
      "F1 Score: 0.6250121142901157\n",
      "Recall: 0.6317142857142858\n",
      "Precision: 0.6188893415297946\n",
      "AUC: 0.6719804535147393\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, LSTM\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "def create_hybrid_model(vocab_size, embedding_dim, maxlen):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    input_incon = Input(shape=(1,))\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
    "    conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
    "    conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "\n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3, input_incon])\n",
    "\n",
    "    lstm_layer = Bidirectional(LSTM(64))(embedding_layer)\n",
    "    concatenated = Concatenate()([lstm_layer, concatenated])\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_incon], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['normalized_scores'][train_index], df['normalized_scores'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Create the hybrid model\n",
    "    model = create_hybrid_model(max_features, embedding_dim, maxlen)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "    model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "              batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict([X_test, incon_test])\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BiGRU/grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e06d882a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ki_shari\\AppData\\Local\\Temp\\ipykernel_16744\\833383917.py:24: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_classifier = KerasClassifier(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 21000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m kf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     37\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mkeras_classifier, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39mkf, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormalized_scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLABEL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Print the best parameters and results\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_result\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:782\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_refit_for_multimetric(scorers)\n\u001b[0;32m    780\u001b[0m     refit_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit\n\u001b[1;32m--> 782\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m fit_params \u001b[38;5;241m=\u001b[39m _check_fit_params(X, fit_params)\n\u001b[0;32m    785\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 443\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 21000]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "def create_model(dropout_rate=0.5, gru_units=64):\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    input2 = Input(shape=(1,))  # Input for incon\n",
    "\n",
    "    embedding_layer = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    "    gru_layer = Bidirectional(GRU(gru_units))(embedding_layer)\n",
    "\n",
    "    concat_layer = Concatenate()([gru_layer, input2])\n",
    "    dropout_layer = Dropout(dropout_rate)(concat_layer)\n",
    "    output = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model with scikit-learn interface\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32],\n",
    "    'reduce_lr__patience': [3, 5],\n",
    "    'reduce_lr__factor': [0.5, 0.1],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'gru_units': [32, 64]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=kf, scoring='accuracy')\n",
    "grid_result = grid_search.fit([X_padded, df['normalized_scores']], df['LABEL'])\n",
    "\n",
    "# Print the best parameters and results\n",
    "print(\"Best parameters:\", grid_result.best_params_)\n",
    "print(\"Best accuracy:\", grid_result.best_score_)\n",
    "print(\"Best estimator:\", grid_result.best_estimator_)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = grid_result.best_estimator_.model\n",
    "\n",
    "# Perform evaluation using the best model\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "def create_input_data(X, normalized_scores):\n",
    "    return [X, normalized_scores]\n",
    "\n",
    "accuracy_scores = []\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    incon_train, incon_test = df['normalized_scores'][train_index], df['normalized_scores'][test_index]\n",
    "    y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=grid_result.best_params_['reduce_lr__patience'],\n",
    "                                 factor=grid_result.best_params_['reduce_lr__factor'], min_lr=0.0001)\n",
    "    best_model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "                   batch_size=grid_result.best_params_['batch_size'], epochs=10,\n",
    "                   callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict([X_test, incon_test])\n",
    "    y_pred_binary = np.round(y_pred).flatten()\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"AUC:\", np.mean(auc_scores))\n",
    "print(\"F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Precision:\", np.mean(precision_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4ef3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n",
      "21000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_padded))\n",
    "print(len(df['normalized_scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08270481",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CNN+BiGRU/grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5e38c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 56s 419ms/step\n",
      "132/132 [==============================] - 68s 513ms/step\n",
      "132/132 [==============================] - 54s 404ms/step\n",
      "132/132 [==============================] - 72s 538ms/step\n",
      "132/132 [==============================] - 74s 555ms/step\n",
      "Params: (64, 3, 32, 32, 0.2)\n",
      "Accuracy: 0.6268571428571429\n",
      "F1 Score: 0.6365573139462102\n",
      "Recall: 0.6545714285714286\n",
      "Precision: 0.6214707233603642\n",
      "AUC: 0.6799805215419501\n",
      "\n",
      "132/132 [==============================] - 61s 458ms/step\n",
      "132/132 [==============================] - 47s 350ms/step\n",
      "132/132 [==============================] - 50s 360ms/step\n",
      "132/132 [==============================] - 42s 307ms/step\n",
      "132/132 [==============================] - 62s 458ms/step\n",
      "Params: (64, 3, 32, 32, 0.5)\n",
      "Accuracy: 0.6243809523809525\n",
      "F1 Score: 0.6295060249148061\n",
      "Recall: 0.6412380952380954\n",
      "Precision: 0.6218784804607078\n",
      "AUC: 0.6735063945578232\n",
      "\n",
      "132/132 [==============================] - 47s 349ms/step\n",
      "132/132 [==============================] - 57s 418ms/step\n",
      "132/132 [==============================] - 68s 497ms/step\n",
      "132/132 [==============================] - 73s 533ms/step\n",
      "132/132 [==============================] - 43s 317ms/step\n",
      "Params: (64, 3, 32, 64, 0.2)\n",
      "Accuracy: 0.6251428571428571\n",
      "F1 Score: 0.6384754780758971\n",
      "Recall: 0.6646666666666666\n",
      "Precision: 0.6179969922232157\n",
      "AUC: 0.6820429251700679\n",
      "\n",
      "132/132 [==============================] - 46s 338ms/step\n",
      "132/132 [==============================] - 51s 373ms/step\n",
      "132/132 [==============================] - 45s 334ms/step\n",
      "132/132 [==============================] - 53s 398ms/step\n",
      "132/132 [==============================] - 52s 391ms/step\n",
      "Params: (64, 3, 32, 64, 0.5)\n",
      "Accuracy: 0.6258571428571429\n",
      "F1 Score: 0.6241437063628522\n",
      "Recall: 0.6221904761904762\n",
      "Precision: 0.627511819183292\n",
      "AUC: 0.6768851247165533\n",
      "\n",
      "132/132 [==============================] - 113s 850ms/step\n",
      "132/132 [==============================] - 124s 930ms/step\n",
      "132/132 [==============================] - 115s 865ms/step\n",
      "132/132 [==============================] - 95s 709ms/step\n",
      "132/132 [==============================] - 99s 740ms/step\n",
      "Params: (64, 3, 64, 32, 0.2)\n",
      "Accuracy: 0.6264285714285714\n",
      "F1 Score: 0.6421864403679138\n",
      "Recall: 0.6720952380952381\n",
      "Precision: 0.6169018447125335\n",
      "AUC: 0.6801703174603174\n",
      "\n",
      "132/132 [==============================] - 92s 689ms/step\n",
      "132/132 [==============================] - 87s 651ms/step\n",
      "132/132 [==============================] - 91s 682ms/step\n",
      "132/132 [==============================] - 89s 665ms/step\n",
      "132/132 [==============================] - 100s 753ms/step\n",
      "Params: (64, 3, 64, 32, 0.5)\n",
      "Accuracy: 0.6318095238095237\n",
      "F1 Score: 0.6213143462851415\n",
      "Recall: 0.6045714285714285\n",
      "Precision: 0.6397766915653331\n",
      "AUC: 0.6798178684807256\n",
      "\n",
      "132/132 [==============================] - 104s 779ms/step\n",
      "132/132 [==============================] - 104s 780ms/step\n",
      "132/132 [==============================] - 104s 784ms/step\n",
      "132/132 [==============================] - 99s 742ms/step\n",
      "132/132 [==============================] - 94s 704ms/step\n",
      "Params: (64, 3, 64, 64, 0.2)\n",
      "Accuracy: 0.6328571428571428\n",
      "F1 Score: 0.6384009336873471\n",
      "Recall: 0.6485714285714286\n",
      "Precision: 0.6292306163794855\n",
      "AUC: 0.6865842857142856\n",
      "\n",
      "132/132 [==============================] - 91s 679ms/step\n",
      "132/132 [==============================] - 95s 712ms/step\n",
      "132/132 [==============================] - 96s 718ms/step\n",
      "132/132 [==============================] - 114s 856ms/step\n",
      "132/132 [==============================] - 115s 865ms/step\n",
      "Params: (64, 3, 64, 64, 0.5)\n",
      "Accuracy: 0.6256666666666667\n",
      "F1 Score: 0.6223217220131737\n",
      "Recall: 0.620095238095238\n",
      "Precision: 0.6289371304324829\n",
      "AUC: 0.6796634013605443\n",
      "\n",
      "132/132 [==============================] - 60s 444ms/step\n",
      "132/132 [==============================] - 97s 730ms/step\n",
      "132/132 [==============================] - 98s 739ms/step\n",
      "132/132 [==============================] - 99s 741ms/step\n",
      "132/132 [==============================] - 105s 787ms/step\n",
      "Params: (64, 4, 32, 32, 0.2)\n",
      "Accuracy: 0.6242857142857142\n",
      "F1 Score: 0.6250009510516238\n",
      "Recall: 0.6292380952380953\n",
      "Precision: 0.6240833531699997\n",
      "AUC: 0.6754026530612245\n",
      "\n",
      "132/132 [==============================] - 114s 854ms/step\n",
      "132/132 [==============================] - 110s 826ms/step\n",
      "132/132 [==============================] - 106s 800ms/step\n",
      "132/132 [==============================] - 113s 848ms/step\n",
      "132/132 [==============================] - 108s 812ms/step\n",
      "Params: (64, 4, 32, 32, 0.5)\n",
      "Accuracy: 0.623952380952381\n",
      "F1 Score: 0.6335242046364745\n",
      "Recall: 0.6519999999999999\n",
      "Precision: 0.6182099111461995\n",
      "AUC: 0.6725320181405896\n",
      "\n",
      "132/132 [==============================] - 106s 796ms/step\n",
      "132/132 [==============================] - 106s 797ms/step\n",
      "132/132 [==============================] - 105s 789ms/step\n",
      "132/132 [==============================] - 116s 876ms/step\n",
      "132/132 [==============================] - 124s 937ms/step\n",
      "Params: (64, 4, 32, 64, 0.2)\n",
      "Accuracy: 0.6292857142857142\n",
      "F1 Score: 0.633007206682705\n",
      "Recall: 0.6411428571428572\n",
      "Precision: 0.6265773412901414\n",
      "AUC: 0.6826392970521542\n",
      "\n",
      "132/132 [==============================] - 120s 900ms/step\n",
      "132/132 [==============================] - 118s 890ms/step\n",
      "132/132 [==============================] - 120s 902ms/step\n",
      "132/132 [==============================] - 111s 839ms/step\n",
      "132/132 [==============================] - 133s 1s/step\n",
      "Params: (64, 4, 32, 64, 0.5)\n",
      "Accuracy: 0.6278571428571429\n",
      "F1 Score: 0.6296861125067255\n",
      "Recall: 0.6335238095238095\n",
      "Precision: 0.6276741682576692\n",
      "AUC: 0.6771599546485261\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     76\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincon_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincon_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     81\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([X_test, incon_test])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, GRU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from itertools import product\n",
    "\n",
    "# Define the model architecture\n",
    "def create_hybrid_model(vocab_size, embedding_dim, maxlen, filters, kernel_size, gru_units, dense_units, dropout_rate):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    input_incon = Input(shape=(1,))\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv2 = Conv1D(filters, kernel_size+1, activation='relu')(embedding_layer)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv3 = Conv1D(filters, kernel_size+2, activation='relu')(embedding_layer)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "\n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3, input_incon])\n",
    "\n",
    "    gru_layer = Bidirectional(GRU(gru_units))(embedding_layer)\n",
    "    concatenated = Concatenate()([gru_layer, concatenated])\n",
    "\n",
    "    dense1 = Dense(dense_units, activation='relu')(concatenated)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dropout1 = Dropout(dropout_rate)(dense1)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_incon], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "max_features = max_features\n",
    "embedding_dim = 300\n",
    "maxlen = 100\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=maxlen)\n",
    "\n",
    "param_grid = {\n",
    "    'filters': [64, 128],\n",
    "    'kernel_size': [3, 4, 5],\n",
    "    'gru_units': [32, 64],\n",
    "    'dense_units': [32, 64],\n",
    "    'dropout_rate': [0.2, 0.5]\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for params in product(*param_grid.values()):\n",
    "    filters, kernel_size, gru_units, dense_units, dropout_rate = params\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "        X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "        incon_train, incon_test = df['normalized_scores'][train_index], df['normalized_scores'][test_index]\n",
    "        y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "        # Create the hybrid model\n",
    "        model = create_hybrid_model(max_features, embedding_dim, maxlen, filters, kernel_size, gru_units, dense_units, dropout_rate)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "        model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "                  batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict([X_test, incon_test])\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "        f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "        recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "        precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "        auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_params = params\n",
    "\n",
    "    print(\"Params:\", params)\n",
    "    print(\"Accuracy:\", avg_accuracy)\n",
    "    print(\"F1 Score:\", avg_f1)\n",
    "    print(\"Recall:\", avg_recall)\n",
    "    print(\"Precision:\", avg_precision)\n",
    "    print(\"AUC:\", avg_auc)\n",
    "    print()\n",
    "\n",
    "# Print the best hyperparameters and their corresponding accuracy\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "# Retrain the model with the best hyperparameters on the full training dataset\n",
    "best_filters, best_kernel_size, best_gru_units, best_dense_units, best_dropout_rate = best_params\n",
    "X_train, incon_train, y_train = ...\n",
    "model = create_hybrid_model(max_features, embedding_dim, maxlen, best_filters, best_kernel_size, best_gru_units, best_dense_units, best_dropout_rate)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit([X_train, incon_train], y_train, batch_size=16, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test, incon_test, y_test = ...\n",
    "y_pred = model.predict([X_test, incon_test])\n",
    "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "test_f1 = f1_score(y_test, y_pred_binary)\n",
    "test_recall = recall_score(y_test, y_pred_binary)\n",
    "test_precision = precision_score(y_test, y_pred_binary)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test AUC:\", test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39739e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CNN+BiLSTM/grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Bidirectional, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from itertools import product\n",
    "\n",
    "# Define the model architecture\n",
    "def create_hybrid_model(vocab_size, embedding_dim, maxlen, filters, kernel_size, gru_units, dense_units, dropout_rate):\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    input_incon = Input(shape=(1,))\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=maxlen)(input_text)\n",
    "    conv1 = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv2 = Conv1D(filters, kernel_size+1, activation='relu')(embedding_layer)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv3 = Conv1D(filters, kernel_size+2, activation='relu')(embedding_layer)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "\n",
    "    pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "    pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "    pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    concatenated = Concatenate()([pooling1, pooling2, pooling3, input_incon])\n",
    "\n",
    "    lst_layer = Bidirectional(LSTM(gru_units))(embedding_layer)\n",
    "    concatenated = Concatenate()([lst_layer, concatenated])\n",
    "\n",
    "    dense1 = Dense(dense_units, activation='relu')(concatenated)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dropout1 = Dropout(dropout_rate)(dense1)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_incon], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "max_features = max_features\n",
    "embedding_dim = 300\n",
    "maxlen = 100\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=maxlen)\n",
    "\n",
    "param_grid = {\n",
    "    'filters': [64, 128],\n",
    "    'kernel_size': [3, 4, 5],\n",
    "    'gru_units': [32, 64],\n",
    "    'dense_units': [32, 64],\n",
    "    'dropout_rate': [0.2, 0.5]\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for params in product(*param_grid.values()):\n",
    "    filters, kernel_size, lst_units, dense_units, dropout_rate = params\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_padded, df['LABEL']):\n",
    "        X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "        incon_train, incon_test = df['normalized_scores'][train_index], df['normalized_scores'][test_index]\n",
    "        y_train, y_test = df['LABEL'][train_index], df['LABEL'][test_index]\n",
    "\n",
    "        # Create the hybrid model\n",
    "        model = create_hybrid_model(max_features, embedding_dim, maxlen, filters, kernel_size, gru_units, dense_units, dropout_rate)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.0001)\n",
    "        model.fit([X_train, incon_train], y_train, validation_data=([X_test, incon_test], y_test),\n",
    "                  batch_size=16, epochs=10, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict([X_test, incon_test])\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "        f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "        recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "        precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "        auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_params = params\n",
    "\n",
    "    print(\"Params:\", params)\n",
    "    print(\"Accuracy:\", avg_accuracy)\n",
    "    print(\"F1 Score:\", avg_f1)\n",
    "    print(\"Recall:\", avg_recall)\n",
    "    print(\"Precision:\", avg_precision)\n",
    "    print(\"AUC:\", avg_auc)\n",
    "    print()\n",
    "\n",
    "# Print the best hyperparameters and their corresponding accuracy\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "# Retrain the model with the best hyperparameters on the full training dataset\n",
    "best_filters, best_kernel_size, best_lst_units, best_dense_units, best_dropout_rate = best_params\n",
    "#X_train, incon_train, y_train = ...\n",
    "model = create_hybrid_model(max_features, embedding_dim, maxlen, best_filters, best_kernel_size, best_lst_units, best_dense_units, best_dropout_rate)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit([X_train, incon_train], y_train, batch_size=16, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "#X_test, incon_test, y_test = ...\n",
    "y_pred = model.predict([X_test, incon_test])\n",
    "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "test_f1 = f1_score(y_test, y_pred_binary)\n",
    "test_recall = recall_score(y_test, y_pred_binary)\n",
    "test_precision = precision_score(y_test, y_pred_binary)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test AUC:\", test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f800def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
